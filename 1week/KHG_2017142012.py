import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

print("===========================================")
print("=               ì‹¤ìŠµ #1                   =")
print("===========================================")
#ë°ì´í„° ì…‹ ë¸”ëŸ¬ì˜¤ê¸° ë° ê° ì—´ ageì™€ tallë¡œ ì„¤ì •
# csvíŒŒì¼ì´ ê°™ì€ í´ë”ë¼ ê°€ì •
data = pd.read_csv('linear_regression_data01.csv', names=["age", "tall"])
print("data:\n",data)
print(data["age"])

# Data set ì¶œë ¥
plt.plot(data['age'], data['tall'], 'b.', label="tall" )
plt.title("Data set")
plt.xlabel("age")
plt.ylabel("tall", rotation=0)
plt.legend(loc="upper left")
plt.grid(True)
plt.show()

# ê¸°ë³¸ ì„¤ì •
x = np.array(data['age'])
y = np.array(data['tall'])
x_size = x.shape
print("x : age list\n",x)
print("y : tall list\n",y)
print("x shape : ",x_size)

print("\n\n===========================================")
print("=               ì‹¤ìŠµ #2                   =")
print("===========================================")
##### í•´ì„í•´

### normal equation
# xì— bias ì¶”ê°€ : ëª¨ë“  íŠ¹ì„±ì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ í¸í–¥ theta0ì— ê°€ìƒì˜ íŠ¹ì„± 1ì„ ì¶”ê°€
x_bias = np.c_[np.ones(x_size),x]

# theta = (x^T x)^-1 x^T Yë¡œ ì„¸íƒ€ê°’ ê³„ì‚°í•˜ëŠ” ìˆ˜ì‹
x_bias_t = x_bias.T
theta = np.linalg.inv(x_bias_t.dot(x_bias)).dot(x_bias_t).dot(y)
print("[Normal Equation] theta0(í¸í–¥) : ",theta[0], " theta1(ê¸°ìš¸ê¸°) : ",theta[1])

### ìˆ˜í•™ì  ìœ ë„
N = x_size[0] # í–‰ ê°¯ìˆ˜ = ì…ë ¥ ë°ì´í„° ê°¯ìˆ˜
x_avg = (sum(x) / N) # xë°ì´í„° í‰ê·  ê°’

# w0 : ê¸°ìš¸ê¸°
w0_upper = 0 #ë¶„ì
w0_lower = 0 #ë¶„ëª¨

for i in range(N):
    w0_upper += y[i] * (x[i] - x_avg) # Yi * (Xi - avg)
    w0_lower += x[i]**2               # xi ^ 2
W0 = (1/N * w0_upper) / ((1 / N) * w0_lower - x_avg**2)

# w1 : í¸í–¥ bias
W1 = 0
for i in range(N):
    W1 += (y[i] - W0*x[i])   # Yi - ê¸°ìš¸ê¸° * Xi
W1 /= N

print("[ìˆ˜í•™ì  ìœ ë„] W0(ê¸°ìš¸ê¸°) : ", W0, " W1(í¸í–¥) : ",W1)

print("\n\n===========================================")
print("=               ì‹¤ìŠµ #3                   =")
print("===========================================")

# ìˆ˜í•™ì  ìœ ë„ í•´ì„í•´ graph
# ì˜ˆì¸¡ê°’ = W0(ê¸°ìš¸ê¸°) * xë°ì´í„° + W1(í¸í–¥ bias)
y_math_pred = W0 * x + W1
# ê°ê°ì— ëŒ€í•œ ì˜ˆì¸¡ ê°’ & ë ˆì´ë¸” ê°’ & ì˜¤ë¥˜ ê°’
error_rate = []
for i in range(x_size[0]):
    error_rate.append(abs(y_math_pred[i]-y[i])/y[i] *100)
    print("ìˆ˜í•™ì  ìœ ë„ ëª¨ë¸ ê°’: {:.2f} ë°ì´í„° ê°’: {} ì˜¤ë¥˜ìœ¨: {:.2f}%".format(y_math_pred[i], y[i], error_rate[i]))
print("ìµœì¢… ì˜¤ë¥˜ìœ¨ : {:.2f}%".format(np.array(error_rate).mean()))
# ìˆ˜í•™ì  ìœ ë„ í•´ì„í•´ graph
plt.plot(x, y, 'b.', label="tall")
plt.plot(x, y_math_pred, 'r-', label="linear math model")
plt.xlabel("age")
plt.ylabel("tall", rotation=0)
plt.title("Math model")
plt.grid(True)
plt.legend(loc="upper left")
plt.show()
# normal equation graph
# ì˜ˆì¸¡ ê°’ = X theta
y_normal_pred = x_bias.dot(theta)
# ì˜¤ë¥˜ ê°’ = (ì˜ˆì¸¡ ê°’ - ë ˆì´ë¸”) / ë ˆì´ë¸”ì˜ í‰ê· ì„ í¼ì„¼íŠ¸ë¡œ ë‚˜íƒ€ë‚¸ ê°’
print("ì˜¤ë¥˜ìœ¨ : {:.2f}%".format((abs(y_normal_pred - y)/y * 100).mean()))
plt.plot(x, y, 'b.', label="tall")
plt.plot(x, y_normal_pred, 'r-', label="linear n.e model")
plt.xlabel("age")
plt.ylabel("tall", rotation=0)
plt.title("Normal Equation model")
plt.legend(loc="upper left")
plt.grid(True)
plt.show()

print("\n\n===========================================")
print("=               ì‹¤ìŠµ #4                   =")
print("===========================================")
### MSE ###

# ìˆ˜í•™ì  ìœ ë„ mse (Mean Square Error)
# ê° ìš”ì†Œì˜ (ì˜ˆì¸¡ê°’ - ë ˆì´ë¸”ê°’)ì„ ì œê³±ì„ í•˜ê¸° ìœ„í•´ mappingí•˜ì˜€ìŠµë‹ˆë‹¤.
math_mse = list(map(lambda x : x**2, y_math_pred - y))

# x_sizeëŠ” 25,1ë¡œ ê¸°ë³¸ ì„¤ì •ì— ì„¤ì •í•˜ì˜€ê¸° ë•Œë¬¸ì— x_size[0]ì€ 25ì…ë‹ˆë‹¤.
# ê° ìš”ì†Œì˜ í•©ì„ í•œ í›„ sizeë§Œí¼ì„ ë‚˜ëˆ  í‰ê· ì„ êµ¬í–ˆìŠµë‹ˆë‹¤.
math_mse = sum(math_mse)/x_size[0]
print("ìˆ˜í•™ì  ìœ ë„ mse : ", math_mse)

# normal equation mse (Mean Square Error)
# ê° ìš”ì†Œì˜ ê°’ì„ ëº€ í›„ ì œê³±ì„ í•˜ê¸°ìœ„í•´ mapì‚¬ìš©
normal_equation_mse = list(map(lambda x : x**2, y_normal_pred - y))

# ê° ìš”ì†Œ í•©í•œ í›„ ë°ì´íŠ¸ ê°¯ìˆ˜ë§í¼ ë‚˜ëˆ  í‰ê· ì„ êµ¬í–ˆìŠµë‹ˆë‹¤.
normal_equation_mse = sum(normal_equation_mse)/x_size[0]
print("Normal equation mse : ", normal_equation_mse)


print("\n\n===========================================")
print("=               ì‹¤ìŠµ #5                   =")
print("===========================================")
##### ê²½ì‚¬í•˜ê°•ë²•

# ê¸°ë³¸
learning_rate = 0.003
n_iter = 20000
# N = x_size[0] ìœ„ì—ì„œ ì„ ì–¸í•¨

np.random.seed(15) # ì´í›„ ëª¨ë“  ì¶œë ¥ ê°’ì„ ë™ì¼í•˜ê²Œ í•˜ê¸° ìœ„í•´ randê°’ ê³ ì •
gd_theta = np.random.randn(2,) # í‰ê·  0 í‘œì¤€í¸ì°¨ 1ì¸ ì •ê·œë¶„í¬ë”°ë¥´ëŠ” -1~1ì‚¬ì´ê°’ ê°€ì ¸ì˜´
gd_theta *= 3  # -1~1ì˜ ê°’ ì¦í­ì„ ìœ„í•œ ê°’ (ì‹œê·¸ë§ˆê°’)

# math
# W0(ê¸°ìš¸ê¸°)ì¸ì§€ ì•„ë‹Œì§€ë¥¼ í™•ì¸í•˜ì—¬ gradient ê³„ì‚°ì‹œ í•„ìš”í•œ x[i]ê°’ì„ ê³±í•©ì§€ íŒë‹¨í•˜ê¸° ìœ„í•´ whichë³€ìˆ˜ ì¶”ê°€
def gradient(which, w, b):
    result = 0
    for i in range(N):
        # ê¸°ìš¸ê¸° ê³„ì‚°ì‹œ : x[i] * (w * x[i] + b - y[i])  = ë°ì´í„° * (ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ê°’ - ë ˆì´ë¸” ê°’)
        result += (x[i] if which == 0 else 1) * (w*x[i] + b - y[i])
    return 2 / N  * result

# W0 = ê¸°ìš¸ê¸°
# W1 = bias

def math_gd(iteration=n_iter, W = gd_theta):
    W0 = W[1]
    W1 = W[0] 
    for _ in range(iteration):
        # ì´í›„ ê°€ì¤‘ì¹˜ ê°’ = í˜„ì¬ ê°€ì¤‘ì¹˜ ê°’ - learning_rate * ë³€í™”ëŸ‰(ê¸°ìš¸ê¸° ê°’)
        W0 = W0 - learning_rate * gradient(0, W0, W1)
        W1 = W1 - learning_rate * gradient(1, W0, W1)

    return W0, W1

result_math_gd = math_gd()
print("[ìˆ˜ì‹ Gradient] ê¸°ìš¸ê¸° : {}, bias : {}".format(W0, W1))
y_math_gd_pred = result_math_gd[0] * x + result_math_gd[1]


# normal
# ë¹„ìš©í•¨ìˆ˜ì— ëŒ€í•œ í¸ë„í•¨ìˆ˜
# ë¯¸ë¶„ MSE(ğœ½) = 2/N * ğ—ğ‘‡(ğ—ğœ½âˆ’ğ²)
# ë°˜ë³µíšŸìˆ˜ì™€ ì´ˆê¸° thetaê°’ ë°›ìŠµë‹ˆë‹¤.
def normal_gd(iteration=n_iter, normal_th=gd_theta):
    for _ in range(iteration):
        # ê¸°ìš¸ê¸° ê°’ = ë¯¸ë¶„ MSE
        gradient = 2/N * x_bias.T.dot(x_bias.dot(normal_th) - y)
        # ë‹¤ìŒ theta(ê°€ì¤‘ì¹˜)ê°’ = í˜„ì¬ thetaê°’ - learning rate * ë³€í™”ëŸ‰(ê¸°ìš¸ê¸° ê°’)
        normal_th = normal_th - learning_rate * gradient

    return normal_th
result_normal_th = normal_gd()

# í–‰ë ¬ ì‹ì„ í†µí•œ ê°’ì´ë¯€ë¡œ theta0 ëŠ” bias, theta1ì€ ê¸°ìš¸ê¸°ì…ë‹ˆë‹¤.
print("[í–‰ë ¬ Gradient] bias : {}, ê¸°ìš¸ê¸° : {}".format(result_normal_th[0],
     result_normal_th[1]))

print("\n\n===========================================")
print("=               ì‹¤ìŠµ #6                   =")
print("===========================================")

print("ì„ì˜ì˜ ì´ˆê¸° thetaê°’ : ", gd_theta)
print("ì´ˆê¸° learning rate : ",learning_rate)
print("ì´ˆê¸° ë°˜ë³µ ê°’ : ", n_iter)

# ê²½ì‚¬í•˜ê°•ë²• STOP
# ì˜¤ì°¨ í—ˆìš© ë³€ìˆ˜
tolerance = 0.0001
# ì˜¤ì°¨ ê³„ì‚° í•¨ìˆ˜
def isStop(now_th, before_th):
    # í˜„ì¬ thetaì™€ befoe thetaê°’ì— ì ˆëŒ€ê°’ ì‚¬ìš©í•˜ì—¬
    # ê°ê°ì˜ ì°¨ì´ ì¤‘ í° ê°’ì´ ì˜¤ì°¨ í—ˆìš© ê°’ë³´ë‹¤ ì‘ì„ ë•Œ ì°¸ì„ ë°˜í™˜
    th_abs = np.abs(now_th) 
    before_th_abs = np.abs(before_th)
    result = (abs(th_abs - before_th_abs).max() < tolerance)
    return result

##### ê²½ì‚¬í•˜ê°•ë²• ê·¸ë˜í”„
# theataì˜ ë§¤ê°œë³€ìˆ˜ learning rateì— ë”°ë¥¸ ê°’ì˜ ë³€í™”ë¥¼ ë³´ê¸° ìœ„í•œ í•¨ìˆ˜
# show : í•´ë‹¹ í•¨ìˆ˜ì—ì„œ ê·¸ë˜í”„ ì¶œë ¥ê¹Œì§€ í•¨ê»˜ í•  ê²ƒì¸ì§€ ì„¤ì •í•˜ëŠ” ë³€ìˆ˜
def theta_normal_graph(rate=learning_rate, th=gd_theta, show=True):
    # ìµœì¢… theta(ê°€ì¤‘ì¹˜)ì— ëŒ€í•œ ì˜¤ë¥˜ìœ¨ í™•ì¸ì„ ìœ„í•œ ë³€ìˆ˜   
    mse = 0
    
    # ê°€ì¥ ë¨¼ì € ê¸°ë³¸ ë°ì´í„°ë¥¼ ì¶œë ¥
    if show:
        plt.plot(x,y,'y.', label="Data")
    
    # ë°˜ë³µ íšŸìˆ˜ë¥¼ ê³ ì •í•œ ì±„ ì§„í–‰
    for i in range(n_iter):
        # thetaê°’ì˜ ë³€í™”ëŸ‰ì„ ì•Œê¸°ìœ„í•´ ì´ì „ before th ê°’ ì €ì¥
        before_th = th
        
        # í–‰ë ¬ì„ í†µí•œ ê¸°ìš¸ê¸° ê³„ì‚°
        gradient = 2/N * x_bias.T.dot(x_bias.dot(th) - y)
        th = th - rate * gradient
        
        # ê·¸ë˜í”„ë¥¼ ì¶œë ¥í•˜ì§€ ì•Šì„ë•Œ = ê²°ê³¼ ê°’ë§Œ ì›í•  ë•Œ
        if not show: 
            # ë©ˆì¶”ëŠ” í•¨ìˆ˜ ì‘ë™
            if isStop(th, before_th):
                break
        
        # ì´ˆë°˜ 10ë²ˆ ë§ˆì§€ë§‰ 1ë²ˆì— ëŒ€í•œ ì˜ˆì¸¡ ëª¨ë¸ ì¶œë ¥í•˜ê¸° ìœ„í•œ ifë¬¸
        if i < 10 or i >n_iter - 2:
            
            # ì˜ˆì¸¡ ëª¨ë¸ ìƒì„±
            y_pred = x_bias.dot(th)
            
            #ê¸°ë³¸ ê·¸ë˜í”„ ì¶œë ¥ ìŠ¤íƒ€ì¼
            style = "b-"
            legend = None
            
            # ì²˜ìŒê³¼ ë§ˆì§€ë§‰ì€ ë‹¤ë¥¸ ìƒ‰ìœ¼ë¡œ ê·¸ë˜í”„ í‘œì‹œ
            # ì²˜ìŒ = ë¹¨ê°„ìƒ‰,  ì¤‘ê°„ = íŒŒë€ìƒ‰,  ë§ˆì§€ë§‰ = ì´ˆë¡ìƒ‰
            if i == 0:
                style = 'r-'
                legend = "start"
            elif i == n_iter - 1:
                style = 'g-' 
                legend = "End"
                
                # MSE ê°’ ê³„ì‚°
                mse = ((y_pred-y)**2).mean()
            
            if show:
                plt.plot(x, y_pred, style, label=legend)
                plt.legend(loc="upper left", fontsize=10)
    if show:
        plt.title("rate={} & normal_graph".format(rate), fontsize=10)   
        plt.grid(True)
    print("[Learning rate] {}, [bias,ê¸°ìš¸ê¸°] : {}, [MSEê°’] : {:.6f}".format(rate, th, mse))
    return th

#ìœ„ì™€ ë™ì¼í•œ í•¨ìˆ˜ì´ë©° ê³„ì‚° ë°©ë²•ë§Œ ë‹¤ë¦„
# show : í•´ë‹¹ í•¨ìˆ˜ì—ì„œ ê·¸ë˜í”„ ì¶œë ¥ê¹Œì§€ í•¨ê»˜ í•  ê²ƒì¸ì§€ ì„¤ì •í•˜ëŠ” ë³€ìˆ˜
def theta_math_graph(rate=learning_rate, th=gd_theta ,show=True):
    # ìµœì¢… theta(ê°€ì¤‘ì¹˜)ì— ëŒ€í•œ ì˜¤ë¥˜ìœ¨ í™•ì¸ì„ ìœ„í•œ ë³€ìˆ˜   
    mse = 0
    
    # ê° ê°’ ë§¤ì¹­
    W0 = th[1]; W1 = th[0]
    
    # ê°€ì¥ ë¨¼ì € ê¸°ë³¸ ë°ì´í„°ë¥¼ ì¶œë ¥
    if show:
        plt.plot(x,y,'y.', label="Data")
    
    # ë°˜ë³µ íšŸìˆ˜ë¥¼ ê³ ì •í•œ ì±„ ì§„í–‰
    for i in range(n_iter):  
        # ìˆ˜ì‹ í†µí•œ ê¸°ìš¸ê¸° ê³„ì‚°
        before_th = np.array([W0, W1])
        
        W0 = W0 - rate * gradient(0, W0, W1)
        W1 = W1 - rate * gradient(1, W0, W1)
        
        if not show: 
            # ë©ˆì¶”ëŠ” í•¨ìˆ˜ ì‘ë™
            if isStop(np.array([W0,W1]), before_th):
                break
        
        # ì´ˆë°˜ 10ë²ˆ ë§ˆì§€ë§‰ 1ë²ˆì— ëŒ€í•œ ì˜ˆì¸¡ ëª¨ë¸ ì¶œë ¥í•˜ê¸° ìœ„í•œ ifë¬¸
        if i < 10 or i >n_iter - 2:
            # ì˜ˆì¸¡ ëª¨ë¸ ìƒì„±
            y_pred = W0 * x + W1
            
            #ê¸°ë³¸ ê·¸ë˜í”„ ì¶œë ¥ ìŠ¤íƒ€ì¼
            style = "b-"
            legend = None
            
            # ì²˜ìŒê³¼ ë§ˆì§€ë§‰ì€ ë‹¤ë¥¸ ìƒ‰ìœ¼ë¡œ ê·¸ë˜í”„ í‘œì‹œ
            # ì²˜ìŒ = ë¹¨ê°„ìƒ‰,  ì¤‘ê°„ = íŒŒë€ìƒ‰,  ë§ˆì§€ë§‰ = ì´ˆë¡ìƒ‰
            if i == 0:
                style = 'r-'
                legend = "start"
            elif i == n_iter - 1:
                style = 'g-' 
                legend = "End"
                
                # ì—ëŸ¬ ê°’ ê³„ì‚°
                mse = ((y_pred-y)**2).mean()
            if show:    
                plt.plot(x, y_pred, style, label=legend)
                plt.legend(loc="upper left", fontsize = 8)
    if show:
        plt.title("rate={} & math_graph".format(rate), fontsize = 10)   
        plt.grid(True)
    
    print("[Learning rate] {}, [bias,ê¸°ìš¸ê¸°] {}, [MSEê°’] : {:.6f}".format(rate,[W1,W0],mse))
    return [W0,W1]

def print_rate(func):    
    plt.figure(figsize=(20,10))
    plt.subplot(231); th = func(rate=0.01)
    plt.subplot(232); th = func(rate=0.005)
    plt.subplot(233); th = func(rate=0.001)
    plt.subplot(234); th = func(rate=0.0005)
    plt.subplot(235); th = func(rate=0.0001); plt.xlabel("x")
    plt.subplot(236); th = func(rate=0.00005)
    plt.show()
    plt.figure(figsize=(20,10))
    plt.subplot(231); th = func(rate=0.001)
    plt.subplot(232); th = func(rate=0.002)
    plt.subplot(233); th = func(rate=0.0025)
    plt.subplot(234); th = func(rate=0.003)
    plt.subplot(235); th = func(rate=0.004); plt.xlabel("x")
    plt.subplot(236); th = func(rate=0.005)
    plt.show()

# í•¨ìˆ˜ë¡œ ì¶œë ¥ ì œì–´
print("MATH:")
print_rate(theta_math_graph)
print("NORMAL:")
print_rate(theta_normal_graph)


print("\n\n===========================================")
print("=               ì‹¤ìŠµ #7                   =")
print("===========================================")

# ë°˜ë³µ íšŸìˆ˜ì— ë”°ë¥¸ ê²°ê³¼ ê°’
def theta_step_stop_graph(rate=learning_rate, th=gd_theta):
    # ê·¸ë˜í”„ ì¶œë ¥ì„ ìœ„í•´ ì¼ì • êµ¬ê°„ë§ˆë‹¤ ì¶œë ¥í•˜ê¸°ìœ„í•´ ì„ ì–¸
    step = 200
    
    # ê·¸ë˜í”„ì˜ xì¶•ì„ ì„¤ì •í•˜ê¸° ìœ„í•´ ì„ ì–¸
    # 0ë²ˆì€ í–‰ë ¬ ì—°ì‚°ì‹œ ì˜¤ì°¨ í—ˆìš© ê°’ë³´ë‹¤ ì‘ì•„ ë©ˆì¶œë•Œê¹Œì°Œ ì¶”ê°€
    # 1ë²ˆì€ ìˆ˜ì‹ ì—°ì‚°ì‹œ ì˜¤ì°¨ í—ˆìš© ê°’ë³´ë‹¤ ì‘ì•„ ë©ˆì¶œë•Œê¹Œì°Œ ì¶”ê°€
    x_step = [[],[]]
    
    # ë©ˆì¶”ê²Œ ë˜ë©´ ê·¸ ì´í›„ëŠ” ê·¸ë˜í”„ ì¶œë ¥í•˜ê¸° ìœ„í•œ ë°°ì—´ ê°’ì— ì¶”ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤.
    # 0ë²ˆ = í–‰ë ¬
    # 1ë²ˆ = ìˆ˜ì‹
    stop = [False, False]
    
    # theataê°’ê³¼ Wê°’ ìì²´ê°€ ë°˜ëŒ€ë¡œ ë˜ì–´ìˆë‹¤
    # theta[0]ëŠ” bias, theta[1]ëŠ” ê¸°ìš¸ê¸° ê°’, W0 = ê¸°ìš¸ê¸°, W1 = bias
    th_math = th[::-1]
    
    # ê° ê·¸ë˜í”„ ì¶œë ¥ì„ ìœ„í•œ ë°°ì—´ 
    th_normal_list = []
    th_math_list = []
    mse_normal_list = []
    mse_math_list = []
    
    for i in range(1,n_iter,step):
        before_th_normal = th
        before_th_math   = th_math
        
        # í—ˆìš© ê°’ ë³´ë‹¤ í´ë•Œ ë™ì‘
        if not stop[0]:
            # theataê°’ ì¶”ê°€ í›„ ê²½ì‚¬í•˜ê°•ë²• ì ìš©
            th_normal_list.append(th)
            th = normal_gd(i, th)
            # í•´ë‹¹í•˜ëŠ” iì¢Œí‘œ ê°’ ì¶”ê°€
            x_step[0].append(i)
            
            # mse normal eq
            y_pred = x_bias.dot(th)
            mse = ((y_pred - y)**2).mean()
            mse_normal_list.append(mse)
        
        if not stop[1]:
            # thetaê°’ ì¶”ê°€ í›„ ê²½ì‚¬í•˜ê°•ë²• ì ìš©
            th_math_list.append(th_math)
            th_math = math_gd(i, th_math)
            # í•´ë‹¹í•˜ëŠ” iì¢Œí‘œ ê°’ ì¶”ê°€
            x_step[1].append(i)
            
            # mse math 
            y_pred = th_math[0] * x + th_math[1]
            mse = ((y_pred - y)**2).mean()
            mse_math_list.append(mse)
        
        # ë§¤ ë°˜ë³µë§ˆë‹¤ í•´ë‹¹ thetaê°’ê³¼ ì´ì „ thetaê°’ì˜ ë³€í™”ìœ¨ì„ ê°ì§€í•˜ì—¬ ê²°ê³¼ ì¶œë ¥
        if isStop(th, before_th_normal):
            stop[0] = True
        
        if isStop(th_math,before_th_math):
            stop[1] = True
        
        # ëª¨ë‘ ë” ì´ìƒ ë³€í™”ê°€ ì—†ë‹¤ë©´ ëë‚¸ë‹¤.
        if stop[0] and stop[1]:
            break
        
    # theataê°’ì„ ê°€ì§€ëŠ” ë¦¬ìŠ¤íŠ¸ëŠ” í•˜ë‚˜ì˜ ì¸ë±ìŠ¤ë§ˆë‹¤ [bias, ê¸°ìš¸ê¸°]ì²˜ëŸ¼ ì¡´ì¬í•˜ê¸°ì—
    # ë¦¬ìŠ¤íŠ¸ë¥¼ transposeí•˜ì—¬ í•˜ë‚˜ì˜ ë°°ì—´ë§ˆë‹¤ ê°ê°ì˜ ê°’ë§Œ ëª¨ìœ¼ê²Œ í•œë‹¤.
    th_normal_list = np.array(th_normal_list).T
    th_math_list = np.array(th_math_list).T
    plt.figure(figsize=(20,10))
    plt.plot(x_step[0], th_normal_list[1], 'b-o', label="normal gradient : w")
    plt.plot(x_step[1], th_math_list[0], 'b-+', label="math gradient : w")
    plt.plot(x_step[0], th_normal_list[0], 'r-o', label="normal bias : b")
    plt.plot(x_step[1], th_math_list[1], 'r-+', label="math bias : b")
    plt.plot(x_step[0], mse_normal_list, 'g-', label="normal eq mse")
    plt.plot(x_step[1], mse_math_list, '-o', color="#000000", label="math eq mse")
    plt.xlabel("step", fontsize = 15)
    plt.ylim(-5,15)
    plt.xlim(0,n_iter)
    plt.legend(loc="upper right", fontsize=18)
    plt.title("rate={}".format(rate), fontsize = 15)   
    plt.show()
    return th

theta_step_stop_graph()

print("\n\n===========================================")
print("=               ì‹¤ìŠµ #8                   =")
print("===========================================")

###### ì‹¤ìŠµ #8
# í›ˆë ¨ ë°ì´í„°   x, y
# í•´ì„í•´
math_model = y_math_pred
normal_model = y_normal_pred
# ê²½ì‚¬í•˜ê°•ë²•
best_math_th = theta_math_graph(show=False)
best_normal_th = theta_normal_graph(show=False)

math_gd_model = best_math_th[0] *x + best_math_th[1]
normal_gd_model = x_bias.dot(best_normal_th)

plt.figure(figsize=(20,10))
plt.plot(x, y, 'y.',label="Train Data")
plt.plot(x, normal_model, 'r-o',label="Normal Equation")
plt.plot(x, math_model, 'b-o',label="Math Equation")
plt.plot(x, math_gd_model, 'b--+',label="Math Gradient")
plt.plot(x, normal_gd_model, 'r--+',label="Normal Gradient")
plt.xlabel("age", fontsize=15)
plt.ylabel("tall", rotation=0, fontsize=15)
plt.legend(loc="upper left", fontsize=18)
plt.title("Total, Prac #8", fontsize=18)
plt.show()